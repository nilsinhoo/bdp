name: spark-ols

services:
  # -------- LOCAL PROFILE: nur Jupyter, Spark local[*] --------
  jupyter-local:
    profiles: ["local"]
    image: jupyter/pyspark-notebook:latest
    container_name: spark-jupyter-local
    hostname: spark-jupyter-local
    environment:
      - PYSPARK_PYTHON=python
      # vermeidet "<unresolved>" beim Start
      - SPARK_LOCAL_IP=spark-jupyter-local
      - SPARK_DRIVER_BIND_ADDRESS=0.0.0.0
      - SPARK_DRIVER_HOST=spark-jupyter-local
      # spark-measure JAR in den Classpath der PySpark-Session hängen
      - PYSPARK_SUBMIT_ARGS=--jars /opt/jars/spark-measure_2.12-0.26.jar pyspark-shell
    # JAR beim Start holen (falls nicht vorhanden), dann Jupyter starten
    command: >
      bash -lc "
        pip install -q pyspark==3.5.0 sparkmeasure==0.26.0 &&
        mkdir -p /opt/jars &&
        if [ ! -f /opt/jars/spark-measure_2.12-0.26.jar ]; then
          curl -fsSL -o /opt/jars/spark-measure_2.12-0.26.jar
          https://repo1.maven.org/maven2/ch/cern/sparkmeasure/spark-measure_2.12/0.26/spark-measure_2.12-0.26.jar;
        fi &&
        start-notebook.sh --NotebookApp.token='' --NotebookApp.password=''
      "
    ports:
      - "${JUPYTER_PORT:-18888}:8888"                # Jupyter
      - "${DRIVER_UI_BASE:-14040}-14050:4040-4050"   # Driver UI(s)
    volumes:
      - ./work:/home/jovyan/work
      - ./jars:/opt/jars
    cpus: ${LOCAL_DRIVER_CORES:-6}
    mem_limit: ${LOCAL_DRIVER_MEM_LIMIT:-24g}
    networks: [sparknet]

  # -------- CLUSTER PROFILE: Master + Worker + Jupyter --------
  spark-master:
    profiles: ["cluster"]
    image: bitnami/spark:3.5.0
    container_name: spark-master
    hostname: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "${MASTER_RPC_PORT:-17077}:7077"   # RPC
      - "${MASTER_UI_PORT:-18080}:8080"    # Master UI
    cpus: ${MASTER_CPUS:-1.0}
    mem_limit: ${MASTER_MEM_LIMIT:-4g}
    networks: [sparknet]

  spark-worker:
    profiles: ["cluster"]
    image: bitnami/spark:3.5.0
    hostname: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=${EXECUTOR_CORES:-2}   # pro Worker (Spark-Sicht)
      - SPARK_WORKER_MEMORY=${EXECUTOR_MEM:-4G}   # pro Worker (Spark-Sicht)
    depends_on: [spark-master]
    # keine festen Ports auf 8081 → skalierbar via --scale
    cpus: ${EXECUTOR_CPUS_LIMIT:-2}          # Docker-Limit (pro Worker)
    mem_limit: ${EXECUTOR_MEM_LIMIT:-4g}     # Docker-Limit (pro Worker)
    networks: [sparknet]

  jupyter-cluster:
    profiles: ["cluster"]
    image: jupyter/pyspark-notebook:latest
    container_name: spark-jupyter
    hostname: spark-jupyter
    environment:
      - PYSPARK_PYTHON=python
      - SPARK_LOCAL_IP=spark-jupyter
      - SPARK_DRIVER_BIND_ADDRESS=0.0.0.0
      - SPARK_DRIVER_HOST=spark-jupyter
      # spark-measure JAR in den Classpath der PySpark-Session hängen
      - PYSPARK_SUBMIT_ARGS=--jars /opt/jars/spark-measure_2.12-0.26.jar pyspark-shell
    command: >
      bash -lc "
        pip install -q pyspark==3.5.0 sparkmeasure==0.26.0 &&
        mkdir -p /opt/jars &&
        if [ ! -f /opt/jars/spark-measure_2.12-0.26.jar ]; then
          curl -fsSL -o /opt/jars/spark-measure_2.12-0.26.jar
          https://repo1.maven.org/maven2/ch/cern/sparkmeasure/spark-measure_2.12/0.26/spark-measure_2.12-0.26.jar;
        fi &&
        start-notebook.sh --NotebookApp.token='' --NotebookApp.password=''
      "
    ports:
      - "${JUPYTER_PORT:-18888}:8888"
      - "${DRIVER_UI_BASE:-14040}-14050:4040-4050"
    volumes:
      - ./work:/home/jovyan/work
      - ./jars:/opt/jars
    depends_on:
      - spark-master
      - spark-worker
    cpus: ${DRIVER_CORES:-2}
    mem_limit: ${DRIVER_MEM_LIMIT:-6g}
    networks: [sparknet]

networks:
  sparknet:
    driver: bridge