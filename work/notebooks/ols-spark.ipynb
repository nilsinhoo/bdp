{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5786ac39-5e2c-4eb0-a8ab-0e575aadd7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PY: /opt/conda/bin/python\n",
      "CWD: /home/jovyan/work/notebooks\n",
      "HOSTNAME: spark-jupyter-local\n"
     ]
    }
   ],
   "source": [
    "import sys, os, socket\n",
    "print(\"PY:\", sys.executable)\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"HOSTNAME:\", socket.gethostname())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45fb2a71-71aa-4fb0-ae2d-49e52f2a03a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"17.0.8.1\" 2023-08-24\n",
      "OpenJDK Runtime Environment (build 17.0.8.1+1-Ubuntu-0ubuntu122.04)\n",
      "OpenJDK 64-Bit Server VM (build 17.0.8.1+1-Ubuntu-0ubuntu122.04, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e90504c-61ad-439b-bbe3-bbfda81c7d5d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark: 3.5.0\n",
      "Spark UI: http://spark-jupyter-local:4040\n",
      "Default parallelism: 12\n",
      "Anzahl Partitionen: 12\n",
      "Driver Memory: 8g\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gc\n",
    "import pyspark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "try:\n",
    "    if 'spark' in globals() and spark is not None:\n",
    "        spark.stop()\n",
    "        del spark\n",
    "except Exception as e:\n",
    "    print(\"Fehler beim Stoppen:\", e)\n",
    "\n",
    "try:\n",
    "    sc = SparkContext._active_spark_context\n",
    "    if sc is not None:\n",
    "        sc.stop()\n",
    "except Exception as e:\n",
    "    print(\"Fehler beim Stoppen:\", e)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "MODE = \"local\"\n",
    "TAG = \"-var_fts\"\n",
    "\n",
    "DRIVER_CORES = 2\n",
    "DRIVER_MEMORY = \"4g\"\n",
    "EXECUTOR_CORES = 4\n",
    "EXECUTOR_MEMORY = 8\n",
    "NUM_PARTITIONS = 3 * EXECUTOR_CORES\n",
    "\n",
    "if MODE == \"local\":\n",
    "    spark = (SparkSession.builder\n",
    "         .appName(\"OLS-Baseline\")\n",
    "         .master(f\"local[{EXECUTOR_CORES}]\")\n",
    "         .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "         .config(\"spark.driver.host\", \"spark-jupyter-local\")\n",
    "         .config(\"spark.ui.port\", \"4040\")\n",
    "         .config(\"spark.driver.memory\", f\"{EXECUTOR_MEMORY}g\")\n",
    "         .config(\"spark.default.parallelism\", NUM_PARTITIONS)\n",
    "         .config(\"spark.sql.shuffle.partitions\", NUM_PARTITIONS)\n",
    "         .getOrCreate())\n",
    "elif MODE == \"single\":\n",
    "    spark = (SparkSession.builder\n",
    "         .appName(\"OLS-Single\")\n",
    "         .master(\"spark://spark-master:7077\")\n",
    "         .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "         .config(\"spark.driver.host\", \"spark-jupyter\")\n",
    "         .config(\"spark.ui.port\", \"4040\")\n",
    "         .config(\"spark.executor.cores\", EXECUTOR_CORES)\n",
    "         .config(\"spark.executor.instances\", 1)\n",
    "         .config(\"spark.executor.memory\",  f\"{EXECUTOR_MEMORY}g\")\n",
    "         .config(\"spark.driver.memory\", DRIVER_MEMORY)\n",
    "         .config(\"spark.default.parallelism\", NUM_PARTITIONS)\n",
    "         .config(\"spark.sql.shuffle.partitions\", NUM_PARTITIONS)\n",
    "         .getOrCreate())\n",
    "elif MODE == \"multi\":\n",
    "    spark = (SparkSession.builder\n",
    "         .appName(\"OLS-Multi\")\n",
    "         .master(\"spark://spark-master:7077\")\n",
    "         .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "         .config(\"spark.driver.host\", \"spark-jupyter\")\n",
    "         .config(\"spark.ui.port\", \"4040\")\n",
    "         .config(\"spark.executor.cores\", EXECUTOR_CORES // 2)\n",
    "         .config(\"spark.executor.instances\", 2)\n",
    "         .config(\"spark.executor.memory\", f\"{EXECUTOR_MEMORY // 2}g\")\n",
    "         .config(\"spark.driver.memory\", DRIVER_MEMORY)\n",
    "         .config(\"spark.default.parallelism\", NUM_PARTITIONS)\n",
    "         .config(\"spark.sql.shuffle.partitions\", NUM_PARTITIONS)\n",
    "         .getOrCreate())\n",
    "\n",
    "\n",
    "print(\"PySpark:\", pyspark.__version__)\n",
    "print(\"Spark UI:\", spark.sparkContext.uiWebUrl)\n",
    "print(\"Default parallelism:\", spark.conf.get(\"spark.default.parallelism\"))\n",
    "print(\"Anzahl Partitionen:\", spark.conf.get(\"spark.sql.shuffle.partitions\"))\n",
    "print(\"Driver Memory:\", spark.conf.get(\"spark.driver.memory\"))\n",
    "\n",
    "try:\n",
    "    print(\"Anzahl Worker:\",  spark.conf.get(\"spark.executor.instances\"))\n",
    "    print(\"CPU-Kerne pro Worker:\", spark.conf.get(\"spark.executor.cores\"))\n",
    "    print(\"Memory pro Worker:\", spark.conf.get(\"spark.executor.memory\"))\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1624222-f56a-4349-9668-e7c85bb64634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.functions import array_to_vector\n",
    "\n",
    "def gen_spark_data_fast(n:int, d:int, seed:int, parts:int):\n",
    "    # früh repartitionieren, damit später kein breiter Shuffle nötig ist\n",
    "    df = spark.range(n).repartition(parts)\n",
    "\n",
    "    # Features als Array mit d Zufallswerten\n",
    "    feats_arr = F.array(*[F.randn(seed + j) for j in range(d)])\n",
    "\n",
    "    # Koeffizienten beta = [1.0, 2.0, ..., d.0]\n",
    "    beta = F.array(*[F.lit(float(j + 1)) for j in range(d)])\n",
    "\n",
    "    # Elementweise multiplizieren und aufsummieren: dot = sum_j feats_arr[j]*beta[j]\n",
    "    prod = F.zip_with(feats_arr, beta, lambda x, b: x * b)\n",
    "    dot  = F.aggregate(prod, F.lit(0.0), lambda acc, v: acc + v)\n",
    "\n",
    "    y = dot + 0.1 * F.randn(seed + d)\n",
    "\n",
    "    # für MLlib: Array → Vector\n",
    "    feats_vec = array_to_vector(feats_arr)\n",
    "    return df.select(feats_vec.alias(\"features\"), y.alias(\"y\"))\n",
    "\n",
    "def human_readable_size(num_bytes: int) -> str:\n",
    "    \"\"\"\n",
    "    Wandelt eine Größe in Bytes in einen gut lesbaren String (KB, MB, GB) um.\n",
    "    \"\"\"\n",
    "    if num_bytes < 1024:\n",
    "        return f\"{num_bytes} B\"\n",
    "    elif num_bytes < 1024**2:\n",
    "        return f\"{num_bytes / 1024:.2f} KB\"\n",
    "    elif num_bytes < 1024**3:\n",
    "        return f\"{num_bytes / 1024**2:.2f} MB\"\n",
    "    else:\n",
    "        return f\"{num_bytes / 1024**3:.2f} GB\"\n",
    "\n",
    "def warm_up(spark):\n",
    "    tmp = gen_spark_data_fast(1_000_000, 200, 1, parts=NUM_PARTITIONS).cache()\n",
    "    _ = tmp.agg(F.count(\"*\")).collect()\n",
    "    _ = LinearRegression(featuresCol=\"features\", labelCol=\"y\", regParam=0.0, elasticNetParam=0.0, solver=\"normal\").fit(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec7f2546-e0a8-406d-8098-2026cc4b94b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkmeasure import StageMetrics\n",
    "import pandas as pd\n",
    "\n",
    "def g(k, metrics): \n",
    "    v = metrics.aggregate_stagemetrics().get(k, 0) \n",
    "    return 0 if v is None else float(v)\n",
    "\n",
    "def get_benchmark(i, model, metrics, mode):\n",
    "    summary = model.summary\n",
    "\n",
    "    elapsed_ms              = g(\"elapsedTime\", metrics)\n",
    "    executorRun_ms          = g(\"executorRunTime\", metrics)\n",
    "    cpu_ms                  = g(\"executorCpuTime\", metrics)\n",
    "    jvm_gc_ms               = g(\"jvmGCTime\", metrics)\n",
    "    scheduler_delay_ms      = g(\"schedulerDelay\", metrics)\n",
    "    task_deser_ms           = g(\"taskDeserializationTime\", metrics)\n",
    "    result_ser_ms           = g(\"resultSerializationTime\", metrics)\n",
    "    shuffle_read_bytes      = g(\"shuffleReadBytes\", metrics)\n",
    "    shuffle_write_bytes     = g(\"shuffleWriteBytes\", metrics)\n",
    "    spilled_mem_bytes       = g(\"memoryBytesSpilled\", metrics)\n",
    "    spilled_disk_bytes      = g(\"diskBytesSpilled\", metrics)\n",
    "    bytes_read              = g(\"bytesRead\", metrics)\n",
    "    bytes_written           = g(\"bytesWritten\", metrics)\n",
    "\n",
    "    wall_overhead_ms    = max(0.0, elapsed_ms - executorRun_ms)\n",
    "    nonCPU_overhead_ms  = max(0.0, executorRun_ms - cpu_ms)\n",
    "    sched_ser_gc_ms     = scheduler_delay_ms + task_deser_ms + result_ser_ms + jvm_gc_ms\n",
    "    \n",
    "    row = {\n",
    "        # Kontext\n",
    "        \"n\": i,\n",
    "        \"mode\": mode,\n",
    "\n",
    "        # Modellgüte (falls vorhanden)\n",
    "        \"r2\": getattr(getattr(model, \"summary\", None), \"r2\", None),\n",
    "        \"rmse\": getattr(getattr(model, \"summary\", None), \"rootMeanSquaredError\", None),\n",
    "        \"mae\": getattr(getattr(model, \"summary\", None), \"meanAbsoluteError\", None),\n",
    "        \"explainedVariance\": getattr(getattr(model, \"summary\", None), \"explainedVariance\", None),\n",
    "\n",
    "        # Zeit/Gesamt\n",
    "        \"elapsed_ms\": elapsed_ms,\n",
    "        \"executorRun_ms\": executorRun_ms,\n",
    "        \"cpu_ms\": cpu_ms,\n",
    "        \"gc_ms\": jvm_gc_ms,\n",
    "\n",
    "        # Overheads\n",
    "        \"wall_overhead_ms\": wall_overhead_ms,\n",
    "        \"nonCPU_overhead_ms\": nonCPU_overhead_ms,\n",
    "        \"sched_ser_gc_ms\": sched_ser_gc_ms,\n",
    "\n",
    "        # Overhead-Anteile (falls sinnvoll berechenbar)\n",
    "        \"wall_overhead_pct\": (wall_overhead_ms / elapsed_ms) if elapsed_ms else None,\n",
    "        \"nonCPU_overhead_pct\": (nonCPU_overhead_ms / executorRun_ms) if executorRun_ms else None,\n",
    "\n",
    "        # Datenbewegung\n",
    "        \"bytesRead\": bytes_read,\n",
    "        \"bytesWritten\": bytes_written,\n",
    "        \"shuffleRead\": shuffle_read_bytes,\n",
    "        \"shuffleWrite\": shuffle_write_bytes,\n",
    "        \"spilled\": (spilled_mem_bytes + spilled_disk_bytes),\n",
    "    }\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bba75c2-8179-4162-9a7f-295d74b07354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_variable_datapoints(datapoints, features):\n",
    "    for m in datapoints:\n",
    "        print(f\"Datensatz vom Shape {m} x {features} generieren. Geschätze Größe: {human_readable_size(m * features * 8)}\")\n",
    "        t0 = time.perf_counter()\n",
    "        sdf = gen_spark_data_fast(m, features, SEED, NUM_PARTITIONS).cache()\n",
    "        _ = sdf.agg(F.count(\"*\")).collect()\n",
    "        t1 = time.perf_counter()\n",
    "        print(f\"gen: {t1 - t0:.2f}s, m={m}, n={features}, seed={SEED}\")\n",
    "        gen_times.append(round(t1 - t0, 2))\n",
    "    \n",
    "        lr = LinearRegression(featuresCol=\"features\", labelCol=\"y\", regParam=0.0, elasticNetParam=0.0, solver=\"normal\")\n",
    "        metrics = StageMetrics(spark)\n",
    "        \n",
    "        t0 = time.perf_counter()\n",
    "        metrics.begin()\n",
    "        \n",
    "        model = lr.fit(sdf)\n",
    "    \n",
    "        metrics.end()\n",
    "        t1 = time.perf_counter()\n",
    "    \n",
    "        res = get_benchmark(i=m, model=model, metrics=metrics, mode=MODE)\n",
    "        results.append(res)\n",
    "        \n",
    "        print(f\"fit: {t1 - t0:.2f}s, R2={model.summary.r2:.4f}\\n\")\n",
    "        fit_times.append(round(t1 - t0, 2))\n",
    "\n",
    "def run_variable_features(features, datapoints):\n",
    "    for n in features:\n",
    "        print(f\"Datensatz vom Shape {datapoints} x {n} generieren. Geschätze Größe: {human_readable_size(datapoints * n * 8)}\")\n",
    "        t0 = time.perf_counter()\n",
    "        # sdf = gen_spark_data(n, D, SEED).repartition(NUM_PARTITIONS).persist()\n",
    "        sdf = gen_spark_data_fast(datapoints, n, SEED, NUM_PARTITIONS).cache()\n",
    "        _ = sdf.agg(F.count(\"*\")).collect()\n",
    "        t1 = time.perf_counter()\n",
    "        print(f\"gen: {t1 - t0:.2f}s, m={datapoints}, n={n}, seed={SEED}\")\n",
    "        gen_times.append(round(t1 - t0, 2))\n",
    "    \n",
    "        lr = LinearRegression(featuresCol=\"features\", labelCol=\"y\", regParam=0.0, elasticNetParam=0.0, solver=\"normal\")\n",
    "        metrics = StageMetrics(spark)\n",
    "        \n",
    "        t0 = time.perf_counter()\n",
    "        metrics.begin()\n",
    "        \n",
    "        model = lr.fit(sdf)\n",
    "    \n",
    "        metrics.end()\n",
    "        t1 = time.perf_counter()\n",
    "    \n",
    "        res = get_benchmark(i=n, model=model, metrics=metrics, mode=MODE)\n",
    "        results.append(res)\n",
    "        \n",
    "        print(f\"fit: {t1 - t0:.2f}s, R2={model.summary.r2:.4f}\\n\")\n",
    "        fit_times.append(round(t1 - t0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77b3638e-0519-4dd6-9155-fa795fd1a1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: local\n",
      "Driver Memory: 8g\n",
      "Datensatz vom Shape 100000 x 8 generieren. Geschätze Größe: 6.10 MB\n",
      "gen: 0.44s, m=100000, n=8, seed=42\n",
      "fit: 0.37s, R2=1.0000\n",
      "\n",
      "Datensatz vom Shape 100000 x 16 generieren. Geschätze Größe: 12.21 MB\n",
      "gen: 0.46s, m=100000, n=16, seed=42\n",
      "fit: 0.36s, R2=1.0000\n",
      "\n",
      "Datensatz vom Shape 100000 x 32 generieren. Geschätze Größe: 24.41 MB\n",
      "gen: 0.52s, m=100000, n=32, seed=42\n",
      "fit: 0.34s, R2=1.0000\n",
      "\n",
      "Datensatz vom Shape 100000 x 64 generieren. Geschätze Größe: 48.83 MB\n",
      "gen: 0.73s, m=100000, n=64, seed=42\n",
      "fit: 0.33s, R2=1.0000\n",
      "\n",
      "Datensatz vom Shape 100000 x 128 generieren. Geschätze Größe: 97.66 MB\n",
      "gen: 1.19s, m=100000, n=128, seed=42\n",
      "fit: 0.46s, R2=1.0000\n",
      "\n",
      "Datensatz vom Shape 100000 x 256 generieren. Geschätze Größe: 195.31 MB\n",
      "gen: 1.96s, m=100000, n=256, seed=42\n",
      "fit: 0.74s, R2=1.0000\n",
      "\n",
      "Datensatz vom Shape 100000 x 512 generieren. Geschätze Größe: 390.62 MB\n",
      "gen: 3.45s, m=100000, n=512, seed=42\n",
      "fit: 2.25s, R2=1.0000\n",
      "\n",
      "Datensatz vom Shape 100000 x 1024 generieren. Geschätze Größe: 781.25 MB\n",
      "gen: 7.08s, m=100000, n=1024, seed=42\n",
      "fit: 7.79s, R2=1.0000\n",
      "\n",
      "Datensatz vom Shape 100000 x 2048 generieren. Geschätze Größe: 1.53 GB\n",
      "gen: 13.60s, m=100000, n=2048, seed=42\n",
      "fit: 105.14s, R2=1.0000\n",
      "\n",
      "Zeiten für Generierung: [0.44, 0.46, 0.52, 0.73, 1.19, 1.96, 3.45, 7.08, 13.6]\n",
      "Zeiten für Training: [0.37, 0.36, 0.34, 0.33, 0.46, 0.74, 2.25, 7.79, 105.14]\n",
      "local-var_fts.csv gespeichert\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "DATAPOINTS_FIX = 100000\n",
    "DATAPOINTS_VAR = range(1_000_000, 16_000_000, 1_000_000) # fine\n",
    "FEATURES_FIX = 10\n",
    "FEATURES_VAR = [8, 16, 32, 64, 128, 256, 512, 1_024, 2_048]#, 4_096]\n",
    "# NS = [1_000, 2_000, 5_000, 10_000, 20_000, 50_000, 100_000, 200_000, 500_000, 1_000_000, 2_000_000, 5_000_000, 10_000_000, 20_000_000, 50_000_000]\n",
    "\n",
    "gen_times = []\n",
    "fit_times = []\n",
    "results = []\n",
    "\n",
    "try:\n",
    "    print(\"Mode:\", MODE)\n",
    "    print(\"Driver Memory:\", spark.conf.get(\"spark.driver.memory\"))\n",
    "    print(\"Anzahl Worker:\",  spark.conf.get(\"spark.executor.instances\"))\n",
    "    print(\"CPU-Kerne pro Worker:\", spark.conf.get(\"spark.executor.cores\"))\n",
    "    print(\"Memory pro Worker:\", spark.conf.get(\"spark.executor.memory\"))\n",
    "    print(\"\\n\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "warm_up(spark)\n",
    "# run_variable_datapoints(DATAPOINTS_VAR, FEATURES_FIX)\n",
    "run_variable_features(FEATURES_VAR, DATAPOINTS_FIX)\n",
    "\n",
    "print(\"Zeiten für Generierung:\", gen_times)\n",
    "print(\"Zeiten für Training:\", fit_times)\n",
    "\n",
    "pd.DataFrame(results).to_csv(f\"../stats/{MODE}{TAG}.csv\", index=False)\n",
    "print(f\"{MODE}{TAG}.csv gespeichert\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
