{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5786ac39-5e2c-4eb0-a8ab-0e575aadd7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PY: /opt/conda/bin/python\n",
      "CWD: /home/jovyan/work/notebooks\n",
      "HOSTNAME: spark-jupyter\n"
     ]
    }
   ],
   "source": [
    "import sys, os, socket\n",
    "print(\"PY:\", sys.executable)\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"HOSTNAME:\", socket.gethostname())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45fb2a71-71aa-4fb0-ae2d-49e52f2a03a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"17.0.8.1\" 2023-08-24\n",
      "OpenJDK Runtime Environment (build 17.0.8.1+1-Ubuntu-0ubuntu122.04)\n",
      "OpenJDK 64-Bit Server VM (build 17.0.8.1+1-Ubuntu-0ubuntu122.04, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e90504c-61ad-439b-bbe3-bbfda81c7d5d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark: 3.5.0\n",
      "Spark UI: http://spark-jupyter:4040\n",
      "Default parallelism: 12\n",
      "Anzahl Partitionen: 12\n",
      "Driver Memory: 4g\n",
      "Anzahl Worker: 2\n",
      "CPU-Kerne pro Worker: 2\n",
      "Memory pro Worker: 4g\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "import gc\n",
    "import pyspark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Nach bestehender Spark-Session suchen und stoppen, falls vorhanden\n",
    "try:\n",
    "    if 'spark' in globals() and spark is not None:\n",
    "        spark.stop()\n",
    "        del spark\n",
    "except Exception as e:\n",
    "    print(\"Fehler beim Stoppen:\", e)\n",
    "\n",
    "try:\n",
    "    sc = SparkContext._active_spark_context\n",
    "    if sc is not None:\n",
    "        sc.stop()\n",
    "except Exception as e:\n",
    "    print(\"Fehler beim Stoppen:\", e)\n",
    "\n",
    "# Garbage-Collector aufrufen und Speicher bereinigen \n",
    "gc.collect()\n",
    "\n",
    "# Spark-Modus einstellen und variable Größe festlegen\n",
    "MODE = \"multi\" # \"multi\"\n",
    "TAG = \"-var_fts\" # \"-var_dpts\"\n",
    "VAR = \"n\" # \"m\" \n",
    "\n",
    "# Seed sowie Features und Datenpunkte festlegen (fix und variabel)\n",
    "SEED = 42\n",
    "DATAPOINTS_FIX = 50000\n",
    "DATAPOINTS_VAR = range(500_000, 12_000_000, 500_000) # fine\n",
    "# DATAPOINTS_VAR = range(1_000_000, 6_000_000, 1_000_000) # test\n",
    "FEATURES_FIX = 10\n",
    "FEATURES_VAR = [8, 16, 32, 64, 128, 256, 512, 768, 1_024, 1_536, 2_048]\n",
    "# NS = [1_000, 2_000, 5_000, 10_000, 20_000, 50_000, 100_000, 200_000, 500_000, 1_000_000, 2_000_000, 5_000_000, 10_000_000, 20_000_000, 50_000_000]\n",
    "\n",
    "# Konfigurationen für Spark-Session \n",
    "DRIVER_CORES = 2\n",
    "DRIVER_MEMORY = \"4g\"\n",
    "EXECUTOR_CORES = 4\n",
    "EXECUTOR_MEMORY = 8\n",
    "NUM_PARTITIONS = 3 * EXECUTOR_CORES\n",
    "\n",
    "if MODE == \"local\":\n",
    "    spark = (SparkSession.builder\n",
    "         .appName(\"OLS-Local\")\n",
    "         .master(f\"local[{EXECUTOR_CORES}]\")\n",
    "         .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "         .config(\"spark.driver.host\", \"spark-jupyter-local\")\n",
    "         .config(\"spark.ui.port\", \"4040\")\n",
    "         .config(\"spark.driver.memory\", f\"{EXECUTOR_MEMORY}g\")\n",
    "         .config(\"spark.default.parallelism\", NUM_PARTITIONS)\n",
    "         .config(\"spark.sql.shuffle.partitions\", NUM_PARTITIONS)\n",
    "         .config(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "         .getOrCreate())\n",
    "elif MODE == \"single\":\n",
    "    spark = (SparkSession.builder\n",
    "         .appName(\"OLS-Single\")\n",
    "         .master(\"spark://spark-master:7077\")\n",
    "         .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "         .config(\"spark.driver.host\", \"spark-jupyter\")\n",
    "         .config(\"spark.ui.port\", \"4040\")\n",
    "         .config(\"spark.executor.cores\", EXECUTOR_CORES)\n",
    "         .config(\"spark.executor.instances\", 1)\n",
    "         .config(\"spark.executor.memory\",  f\"{EXECUTOR_MEMORY}g\")\n",
    "         .config(\"spark.driver.memory\", DRIVER_MEMORY)\n",
    "         .config(\"spark.driver.cores\", DRIVER_CORES)\n",
    "         .config(\"spark.default.parallelism\", NUM_PARTITIONS)\n",
    "         .config(\"spark.sql.shuffle.partitions\", NUM_PARTITIONS)\n",
    "         .config(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "         .getOrCreate())\n",
    "elif MODE == \"multi\":\n",
    "    spark = (SparkSession.builder\n",
    "         .appName(\"OLS-Multi\")\n",
    "         .master(\"spark://spark-master:7077\")\n",
    "         .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "         .config(\"spark.driver.host\", \"spark-jupyter\")\n",
    "         .config(\"spark.ui.port\", \"4040\")\n",
    "         .config(\"spark.executor.cores\", EXECUTOR_CORES // 2)\n",
    "         .config(\"spark.executor.instances\", 2)\n",
    "         .config(\"spark.executor.memory\", f\"{EXECUTOR_MEMORY // 2}g\")\n",
    "         .config(\"spark.driver.cores\", DRIVER_CORES)\n",
    "         .config(\"spark.driver.memory\", DRIVER_MEMORY)\n",
    "         .config(\"spark.default.parallelism\", NUM_PARTITIONS)\n",
    "         .config(\"spark.sql.shuffle.partitions\", NUM_PARTITIONS)\n",
    "         .config(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# Cache leeren\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "print(\"PySpark:\", pyspark.__version__)\n",
    "print(\"Spark UI:\", spark.sparkContext.uiWebUrl)\n",
    "print(\"Default parallelism:\", spark.conf.get(\"spark.default.parallelism\"))\n",
    "print(\"Anzahl Partitionen:\", spark.conf.get(\"spark.sql.shuffle.partitions\"))\n",
    "print(\"Driver Memory:\", spark.conf.get(\"spark.driver.memory\"))\n",
    "\n",
    "try:\n",
    "    print(\"Anzahl Worker:\",  spark.conf.get(\"spark.executor.instances\"))\n",
    "    print(\"CPU-Kerne pro Worker:\", spark.conf.get(\"spark.executor.cores\"))\n",
    "    print(\"Memory pro Worker:\", spark.conf.get(\"spark.executor.memory\"))\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1624222-f56a-4349-9668-e7c85bb64634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.functions import array_to_vector\n",
    "\n",
    "# Datensatz generieren und Verteilen auf Partitionen\n",
    "def gen_spark_data_with_repartitioning(n:int, d:int, seed:int, parts:int):\n",
    "    df = spark.range(n).repartition(parts)\n",
    "\n",
    "    feats_arr = F.array(*[F.randn(seed + j) for j in range(d)])\n",
    "\n",
    "    beta = F.array(*[F.lit(float(j + 1)) for j in range(d)])\n",
    "\n",
    "    prod = F.zip_with(feats_arr, beta, lambda x, b: x * b)\n",
    "    dot  = F.aggregate(prod, F.lit(0.0), lambda acc, v: acc + v)\n",
    "\n",
    "    y = dot + 0.1 * F.randn(seed + d)\n",
    "\n",
    "    feats_vec = array_to_vector(feats_arr)\n",
    "    return df.select(feats_vec.alias(\"features\"), y.alias(\"y\"))\n",
    "\n",
    "# Helper für Ausgabe -> geschätzte Größe in Bytes in gut lesbaren String umwandeln \n",
    "def human_readable_size(num_bytes: int):\n",
    "    if num_bytes < 1024:\n",
    "        return f\"{num_bytes} B\"\n",
    "    elif num_bytes < 1024**2:\n",
    "        return f\"{num_bytes / 1024:.2f} KB\"\n",
    "    elif num_bytes < 1024**3:\n",
    "        return f\"{num_bytes / 1024**2:.2f} MB\"\n",
    "    else:\n",
    "        return f\"{num_bytes / 1024**3:.2f} GB\"\n",
    "\n",
    "# Spark \"aufwärmen\"\n",
    "def warm_up(spark):\n",
    "    tmp = gen_spark_data_with_repartitioning(1_000_000, 10, 1, parts=NUM_PARTITIONS).cache()\n",
    "    _ = tmp.agg(F.count(\"*\")).collect()\n",
    "    _ = LinearRegression(featuresCol=\"features\", labelCol=\"y\", regParam=0.0, elasticNetParam=0.0, solver=\"normal\").fit(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec7f2546-e0a8-406d-8098-2026cc4b94b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkmeasure import StageMetrics\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.ml.regression import LinearRegression, LinearRegressionModel\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Helper für Zugriff auf SparkSession\n",
    "def c(key: str, default=None): \n",
    "    spark = SparkSession.getActiveSession()\n",
    "    try:\n",
    "        return spark.conf.get(key, default)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "# Helper für Zugriff auf StageMetrics\n",
    "def g(key: str, metrics: StageMetrics) -> float:\n",
    "    v = metrics.aggregate_stagemetrics().get(key, 0)\n",
    "    return 0.0 if v is None else float(v)\n",
    "\n",
    "def get_theoretical_size_bytes(m: int, n: int, d_size: int = 8):\n",
    "    return (m * n + m) * d_size\n",
    "\n",
    "def get_benchmark(m: int, n: int, t_fit: float, model: LinearRegressionModel, metrics: StageMetrics, mode: str):\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    sc = spark.sparkContext\n",
    "    \n",
    "    elapsed_ms              = g(\"elapsedTime\", metrics)\n",
    "    executorRun_ms          = g(\"executorRunTime\", metrics)\n",
    "    cpu_ms                  = g(\"executorCpuTime\", metrics)\n",
    "    jvm_gc_ms               = g(\"jvmGCTime\", metrics)\n",
    "    scheduler_delay_ms      = g(\"schedulerDelay\", metrics)\n",
    "    task_deser_ms           = g(\"taskDeserializationTime\", metrics)\n",
    "    result_ser_ms           = g(\"resultSerializationTime\", metrics)\n",
    "    shuffle_read_bytes      = g(\"shuffleReadBytes\", metrics)\n",
    "    shuffle_write_bytes     = g(\"shuffleWriteBytes\", metrics)\n",
    "    spilled_mem_bytes       = g(\"memoryBytesSpilled\", metrics)\n",
    "    spilled_disk_bytes      = g(\"diskBytesSpilled\", metrics)\n",
    "    bytes_read              = g(\"bytesRead\", metrics)\n",
    "    bytes_written           = g(\"bytesWritten\", metrics)\n",
    "\n",
    "    t_fit_ms                = t_fit * 1000\n",
    "    num_executors           = EXECUTOR_CORES\n",
    "    wall_overhead_ms        = (t_fit_ms - executorRun_ms / num_executors)\n",
    "    nonCPU_overhead_ms      = max(0.0, executorRun_ms - cpu_ms)\n",
    "    parallel_overhead_ms    = wall_overhead_ms + nonCPU_overhead_ms\n",
    "    wall_overhead_pct       = (wall_overhead_ms / t_fit_ms) if t_fit_ms else None\n",
    "    nonCPU_overhead_pct     = (nonCPU_overhead_ms / executorRun_ms) if executorRun_ms else None\n",
    "    parallel_overhead_pct   = (parallel_overhead_ms / t_fit_ms) if t_fit_ms else None\n",
    "    cpu_efficiency          = (cpu_ms / (num_executors * t_fit_ms)) if (num_executors * t_fit_ms) else None\n",
    "\n",
    "    row = {\n",
    "        # Kontext\n",
    "        \"m_datapoints\": m,\n",
    "        \"n_features\": n,\n",
    "        \"mode\": mode,\n",
    "\n",
    "        # Driver/Executor/Partitionen\n",
    "        \"driver_cores\": c(\"spark.driver.cores\", \"0\"),\n",
    "        \"driver_memory\": c(\"spark.driver.memory\", \"0\"),\n",
    "        \"executor_instances\": c(\"spark.executor.instances\", \"0\"),\n",
    "        \"executor_cores\": c(\"spark.executor.cores\", \"0\"),\n",
    "        \"executor_memory\": c(\"spark.executor.memory\", \"0\"),\n",
    "        \"default_parallelism\": sc.defaultParallelism,\n",
    "        \"shuffle_partitions\": c(\"spark.sql.shuffle.partitions\", \"\"),\n",
    "        \"master\": sc.master,\n",
    "\n",
    "        # Modellgüte\n",
    "        \"r2\": getattr(getattr(model, \"summary\", None), \"r2\", None),\n",
    "        \"rmse\": getattr(getattr(model, \"summary\", None), \"rootMeanSquaredError\", None),\n",
    "        \"mae\": getattr(getattr(model, \"summary\", None), \"meanAbsoluteError\", None),\n",
    "        \"explainedVariance\": getattr(getattr(model, \"summary\", None), \"explainedVariance\", None),\n",
    "\n",
    "        # Zeit/Gesamt\n",
    "        \"t_fit_ms\": t_fit_ms,\n",
    "        \"elapsed_ms\": elapsed_ms,\n",
    "        \"executorRun_ms\": executorRun_ms,\n",
    "        \"cpu_ms\": cpu_ms,\n",
    "        \"gc_ms\": jvm_gc_ms,\n",
    "\n",
    "        # Overheads\n",
    "        \"wall_overhead_ms\": wall_overhead_ms,\n",
    "        \"nonCPU_overhead_ms\": nonCPU_overhead_ms,\n",
    "        \"parallel_overhead_ms\": parallel_overhead_ms,\n",
    "\n",
    "        # Overhead-Anteile\n",
    "        \"wall_overhead_pct\": wall_overhead_pct,\n",
    "        \"nonCPU_overhead_pct\": nonCPU_overhead_pct,\n",
    "        \"parallel_overhead_pct\": parallel_overhead_pct,\n",
    "\n",
    "        # Datenbewegung\n",
    "        \"bytes_read\": bytes_read,\n",
    "        \"bytes_written\": bytes_written,\n",
    "        \"shuffle_read\": shuffle_read_bytes,\n",
    "        \"shuffle_write\": shuffle_write_bytes,\n",
    "        \"spilled\": (spilled_mem_bytes + spilled_disk_bytes),\n",
    "\n",
    "        # Speicher\n",
    "        \"theoretical_size_bytes\": get_theoretical_size_bytes(m, n, d_size=8),\n",
    "    }\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bba75c2-8179-4162-9a7f-295d74b07354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_variable_datapoints(datapoints, features):\n",
    "    # Schleife für iteratives Erhöhen der Datenpunkte\n",
    "    for m in datapoints:\n",
    "        print(f\"Datensatz vom Shape {m} x {features} generieren. Geschätze Größe: {human_readable_size(m * features * 8)}\")\n",
    "\n",
    "        # Datensatz generieren\n",
    "        t0 = time.perf_counter()\n",
    "        sdf = gen_spark_data_with_repartitioning(m, features, SEED, NUM_PARTITIONS).cache()\n",
    "        _ = sdf.agg(F.count(\"*\")).collect()\n",
    "        t1 = time.perf_counter()\n",
    "        \n",
    "        print(f\"gen: {t1 - t0:.2f}s, m={m}, n={features}, seed={SEED}\")\n",
    "        gen_times.append(round(t1 - t0, 2))\n",
    "\n",
    "        # OLS-Regression\n",
    "        lr = LinearRegression(featuresCol=\"features\", labelCol=\"y\", regParam=0.0, elasticNetParam=0.0, solver=\"normal\")\n",
    "        metrics = StageMetrics(spark)\n",
    "\n",
    "        # Modell trainieren und Zeit messen\n",
    "        metrics.begin()\n",
    "        t0 = time.perf_counter()\n",
    "        model = lr.fit(sdf)\n",
    "        t1 = time.perf_counter()\n",
    "        metrics.end()\n",
    "        t_fit = round(t1 - t0, 2)\n",
    "\n",
    "        # Kennzahlen erfassen\n",
    "        res = get_benchmark(m=m, n=features, t_fit=t_fit, model=model, metrics=metrics, mode=MODE)\n",
    "        results.append(res)\n",
    "        \n",
    "        print(f\"fit: {t1 - t0:.2f}s, R2={model.summary.r2:.4f}\\n\")\n",
    "        fit_times.append(round(t1 - t0, 2))\n",
    "\n",
    "\n",
    "def run_variable_features(features, datapoints):\n",
    "    # Schleife für iteratives Erhöhen der Features\n",
    "    for n in features:\n",
    "        print(f\"Datensatz vom Shape {datapoints} x {n} generieren. Geschätze Größe: {human_readable_size(datapoints * n * 8)}\")\n",
    "\n",
    "        # Datensatz generieren\n",
    "        t0 = time.perf_counter()\n",
    "        sdf = gen_spark_data_with_repartitioning(datapoints, n, SEED, NUM_PARTITIONS).cache()\n",
    "        _ = sdf.agg(F.count(\"*\")).collect()\n",
    "        t1 = time.perf_counter()\n",
    "        \n",
    "        print(f\"gen: {t1 - t0:.2f}s, m={datapoints}, n={n}, seed={SEED}\")\n",
    "        gen_times.append(round(t1 - t0, 2))\n",
    "        \n",
    "        # OLS-Regression\n",
    "        lr = LinearRegression(featuresCol=\"features\", labelCol=\"y\", regParam=0.0, elasticNetParam=0.0, solver=\"normal\")\n",
    "        metrics = StageMetrics(spark)\n",
    "\n",
    "        # Modell trainieren und Zeit messen\n",
    "        metrics.begin()\n",
    "        t0 = time.perf_counter()\n",
    "        model = lr.fit(sdf)\n",
    "        t1 = time.perf_counter()\n",
    "        metrics.end()\n",
    "        t_fit = round(t1 - t0, 2)\n",
    "        \n",
    "        # Kennzahlen erfassen\n",
    "        res = get_benchmark(m=datapoints, n=n, t_fit=t_fit, model=model, metrics=metrics, mode=MODE)\n",
    "        results.append(res)\n",
    "        \n",
    "        print(f\"fit: {t1 - t0:.2f}s, R2={model.summary.r2:.4f}\\n\")\n",
    "        fit_times.append(round(t1 - t0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77b3638e-0519-4dd6-9155-fa795fd1a1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: multi\n",
      "Variable Größe: n\n",
      "Driver Memory: 4g\n",
      "Anzahl Worker: 2\n",
      "CPU-Kerne pro Worker: 2\n",
      "Memory pro Worker: 4g\n",
      "\n",
      "\n",
      "Datensatz vom Shape 50000 x 8 generieren. Geschätze Größe: 3.05 MB\n",
      "gen: 0.81s, m=50000, n=8, seed=42\n",
      "fit: 0.64s, R2=1.0000\n",
      "\n",
      "Datensatz vom Shape 50000 x 16 generieren. Geschätze Größe: 6.10 MB\n",
      "gen: 0.57s, m=50000, n=16, seed=42\n",
      "fit: 0.55s, R2=1.0000\n",
      "\n",
      "Datensatz vom Shape 50000 x 32 generieren. Geschätze Größe: 12.21 MB\n",
      "gen: 0.67s, m=50000, n=32, seed=42\n",
      "fit: 0.62s, R2=1.0000\n",
      "\n",
      "Datensatz vom Shape 50000 x 64 generieren. Geschätze Größe: 24.41 MB\n",
      "gen: 0.73s, m=50000, n=64, seed=42\n",
      "fit: 0.53s, R2=1.0000\n",
      "\n",
      "Datensatz vom Shape 50000 x 128 generieren. Geschätze Größe: 48.83 MB\n",
      "gen: 1.08s, m=50000, n=128, seed=42\n",
      "fit: 0.53s, R2=1.0000\n",
      "\n",
      "Datensatz vom Shape 50000 x 256 generieren. Geschätze Größe: 97.66 MB\n",
      "gen: 1.61s, m=50000, n=256, seed=42\n",
      "fit: 0.94s, R2=1.0000\n",
      "\n",
      "Datensatz vom Shape 50000 x 512 generieren. Geschätze Größe: 195.31 MB\n",
      "gen: 3.00s, m=50000, n=512, seed=42\n",
      "fit: 2.05s, R2=1.0000\n",
      "\n",
      "Datensatz vom Shape 50000 x 768 generieren. Geschätze Größe: 292.97 MB\n",
      "gen: 3.98s, m=50000, n=768, seed=42\n",
      "fit: 3.55s, R2=1.0000\n",
      "\n",
      "Datensatz vom Shape 50000 x 1024 generieren. Geschätze Größe: 390.62 MB\n",
      "gen: 5.12s, m=50000, n=1024, seed=42\n",
      "fit: 5.99s, R2=1.0000\n",
      "\n",
      "Datensatz vom Shape 50000 x 1536 generieren. Geschätze Größe: 585.94 MB\n",
      "gen: 7.29s, m=50000, n=1536, seed=42\n",
      "fit: 21.86s, R2=1.0000\n",
      "\n",
      "Datensatz vom Shape 50000 x 2048 generieren. Geschätze Größe: 781.25 MB\n",
      "gen: 9.04s, m=50000, n=2048, seed=42\n",
      "fit: 47.72s, R2=1.0000\n",
      "\n",
      "Zeiten für Generierung: [0.81, 0.57, 0.67, 0.73, 1.08, 1.61, 3.0, 3.98, 5.12, 7.29, 9.04]\n",
      "Zeiten für Training: [0.64, 0.55, 0.62, 0.53, 0.53, 0.94, 2.05, 3.55, 5.99, 21.86, 47.72]\n",
      "multi-var_fts.csv gespeichert\n"
     ]
    }
   ],
   "source": [
    "gen_times = []\n",
    "fit_times = []\n",
    "results = []\n",
    "\n",
    "try:\n",
    "    print(\"Mode:\", MODE)\n",
    "    print(\"Variable Größe:\", VAR)\n",
    "    print(\"Driver Memory:\", spark.conf.get(\"spark.driver.memory\"))\n",
    "    print(\"Anzahl Worker:\",  spark.conf.get(\"spark.executor.instances\"))\n",
    "    print(\"CPU-Kerne pro Worker:\", spark.conf.get(\"spark.executor.cores\"))\n",
    "    print(\"Memory pro Worker:\", spark.conf.get(\"spark.executor.memory\"))\n",
    "    print(\"\\n\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Spark \"aufwärmen\"\n",
    "warm_up(spark)\n",
    "\n",
    "# Versuchsreihe für eingestellte variable Größe ausführen\n",
    "if VAR == \"m\":\n",
    "    run_variable_datapoints(DATAPOINTS_VAR, FEATURES_FIX)\n",
    "elif VAR == \"n\":\n",
    "    run_variable_features(FEATURES_VAR, DATAPOINTS_FIX)\n",
    "\n",
    "print(\"Zeiten für Generierung:\", gen_times)\n",
    "print(\"Zeiten für Training:\", fit_times)\n",
    "\n",
    "pd.DataFrame(results).to_csv(f\"../stats/{MODE}{TAG}.csv\", index=False)\n",
    "print(f\"{MODE}{TAG}.csv gespeichert\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
